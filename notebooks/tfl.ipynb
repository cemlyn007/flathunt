{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53164d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTHON_GIL\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "import dotenv\n",
    "\n",
    "import london.roads\n",
    "import tfl.api\n",
    "import tfl.exceptions\n",
    "import tfl.models\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec924d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.StreamHandler()\n",
    "\n",
    "\n",
    "class LogFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return \"tfl.api\" in record.module\n",
    "\n",
    "\n",
    "# logging.getLogger().addFilter()\n",
    "handler.addFilter(LogFilter())\n",
    "logging.getLogger().addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89530b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_client = tfl.api.Tfl(app_key=os.environ[\"FLATHUNT__TFL_API_KEY\"])\n",
    "\n",
    "stations_facilities = await tf_client.get_stations_facilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07779bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stations_facilities.stations.station[0].model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f196ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: using this gives me the coordinates and the station IDs so that I can get the timetables!\n",
    "other_result = await tf_client.get_stop_points_by_mode(tfl.models.ModeId.TUBE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc66c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(other_result[0].model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78786eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await tf_client.get_timetable(\n",
    "    from_stop_point_id=\"940GZZLUEUS\",\n",
    "    station_id=\"victoria\",\n",
    "    direction=tfl.api.Direction.INBOUND,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I have the facitilies\n",
    "# Now I need to get the bus stops - Yue Says NO BUSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81fa7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ee0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "for route in result.timetable.routes:\n",
    "    for station_interval in route.station_intervals:\n",
    "        for interval in station_interval.intervals:\n",
    "            if interval.stop_id == \"940GZZLUGPK\":  # Green Park\n",
    "                print(f\"Time to Green Park: {interval.time_to_arrival} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for route in result.timetable.routes:\n",
    "    for schedule in route.schedules:\n",
    "        print(f\"Schedule: {schedule.name}\")\n",
    "        for period in schedule.periods or []:\n",
    "            if period.frequency:\n",
    "                print(\n",
    "                    f\"  {period.from_time.hour}:{period.from_time.minute} - \"\n",
    "                    f\"{period.to_time.hour}:{period.to_time.minute}: \"\n",
    "                    f\"every {period.frequency.highest_frequency}-{period.frequency.lowest_frequency} mins\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load transport point features (stations)\n",
    "transport_gdf = gpd.read_file(\n",
    "    \"/home/cemlyn/Downloads/greater-london-251126-free.shp/gis_osm_transport_free_1.shp\"\n",
    ")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Total transport features: {len(transport_gdf)}\")\n",
    "print(f\"\\nColumns: {transport_gdf.columns.tolist()}\")\n",
    "print(\"\\nUnique fclass values:\")\n",
    "print(transport_gdf[\"fclass\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b1548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for railway stations\n",
    "railway_stations = transport_gdf[transport_gdf[\"fclass\"] == \"railway_station\"].copy()\n",
    "\n",
    "print(f\"Total railway stations: {len(railway_stations)}\")\n",
    "print(\"\\nFirst few stations:\")\n",
    "print(railway_stations[[\"name\", \"osm_id\"]].head(10))\n",
    "\n",
    "# Get geometry info\n",
    "railway_stations[\"lon\"] = railway_stations.geometry.x\n",
    "railway_stations[\"lat\"] = railway_stations.geometry.y\n",
    "\n",
    "print(\"\\nSample coordinates:\")\n",
    "print(railway_stations[[\"name\", \"lon\", \"lat\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "railway_stations[[\"name\", \"lon\", \"lat\"]].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f11159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also load railway lines to see if we can identify tube lines\n",
    "railways_gdf = gpd.read_file(\n",
    "    \"/home/cemlyn/Downloads/greater-london-251126-free.shp/gis_osm_railways_free_1.shp\"\n",
    ")\n",
    "\n",
    "print(f\"Total railway features: {len(railways_gdf)}\")\n",
    "print(f\"\\nColumns: {railways_gdf.columns.tolist()}\")\n",
    "print(\"\\nUnique fclass values:\")\n",
    "print(railways_gdf[\"fclass\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358fa44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter subway lines\n",
    "subway_lines = railways_gdf[railways_gdf[\"fclass\"] == \"subway\"].copy()\n",
    "\n",
    "print(f\"Total subway line segments: {len(subway_lines)}\")\n",
    "print(\"\\nUnique subway lines:\")\n",
    "unique_lines = subway_lines[\"name\"].dropna().unique()\n",
    "print(f\"Found {len(unique_lines)} named subway lines\")\n",
    "for line in sorted(unique_lines)[:20]:  # Show first 20\n",
    "    print(f\"  - {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e08c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive tube stations dataset\n",
    "# We'll use the railway stations and identify which ones are tube stations\n",
    "# by checking proximity to subway lines\n",
    "\n",
    "# First, let's create a buffered version of subway lines\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "# Union all subway lines and create a buffer (100m)\n",
    "subway_union = unary_union(subway_lines.geometry)\n",
    "subway_buffer = subway_union.buffer(0.001)  # roughly 100m in degrees\n",
    "\n",
    "# Find railway stations near subway lines\n",
    "railway_stations[\"near_subway\"] = railway_stations.geometry.within(subway_buffer)\n",
    "\n",
    "tube_stations = railway_stations[railway_stations[\"near_subway\"]].copy()\n",
    "\n",
    "print(f\"Total railway stations: {len(railway_stations)}\")\n",
    "print(f\"Tube stations (near subway lines): {len(tube_stations)}\")\n",
    "print(\"\\nSample tube stations:\")\n",
    "print(tube_stations[[\"name\", \"lon\", \"lat\"]].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean dataset for tube stations\n",
    "tube_stations_clean = tube_stations[[\"name\", \"osm_id\", \"lon\", \"lat\"]].copy()\n",
    "tube_stations_clean = tube_stations_clean.sort_values(\"name\").reset_index(drop=True)\n",
    "\n",
    "# Create a dictionary for easy lookup\n",
    "tube_stations_dict = {\n",
    "    row[\"name\"]: {\"osm_id\": row[\"osm_id\"], \"lat\": row[\"lat\"], \"lon\": row[\"lon\"]}\n",
    "    for _, row in tube_stations_clean.iterrows()\n",
    "    if row[\"name\"]  # Filter out any None names\n",
    "}\n",
    "\n",
    "print(f\"Loaded {len(tube_stations_dict)} tube stations\")\n",
    "print(\"\\nExample stations:\")\n",
    "for i, (name, info) in enumerate(list(tube_stations_dict.items())[:5]):\n",
    "    print(f\"  {name}: ({info['lat']:.6f}, {info['lon']:.6f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2832eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = london.roads.RoadGraphLoader().load_from_shapefile(\n",
    "    \"/home/cemlyn/Downloads/greater-london-251126-free.shp/gis_osm_roads_free_1.shp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b586174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(graph.nodes.keys())[0]\n",
    "graph.nodes[\"-0.193124,51.601725\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f8d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"Calculate the great circle distance between two points on earth (in km)\n",
    "\n",
    "    All inputs can be scalars or numpy arrays. Arrays will be broadcast together.\n",
    "    \"\"\"\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat2 = np.radians(lat2)\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6371 * c\n",
    "    return km\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c657a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "node_ids = list(graph.nodes.keys())\n",
    "node_lons = np.array([graph.nodes[nid][\"lon\"] for nid in node_ids])\n",
    "node_lats = np.array([graph.nodes[nid][\"lat\"] for nid in node_ids])\n",
    "\n",
    "station_lons = railway_stations[\"lon\"].values\n",
    "station_lats = railway_stations[\"lat\"].values\n",
    "\n",
    "# Process in chunks to avoid memory issues\n",
    "chunk_size = 8192\n",
    "closest_station_indices = np.empty(len(node_ids), dtype=np.int64)\n",
    "closest_station_distances = np.empty(len(node_ids), dtype=np.float64)\n",
    "\n",
    "with tqdm.tqdm(total=len(node_ids)) as pbar:\n",
    "    for i in range(0, len(node_ids), chunk_size):\n",
    "        chunk_end = min(i + chunk_size, len(node_ids))\n",
    "        chunk_lons = node_lons[i:chunk_end]\n",
    "        chunk_lats = node_lats[i:chunk_end]\n",
    "\n",
    "        # Compute distances for this chunk\n",
    "        distances = haversine(\n",
    "            chunk_lons[:, np.newaxis],\n",
    "            chunk_lats[:, np.newaxis],\n",
    "            station_lons[np.newaxis, :],\n",
    "            station_lats[np.newaxis, :],\n",
    "        )\n",
    "        if distances.shape != (chunk_end - i, station_lons.shape[0]):\n",
    "            raise ValueError(\"Fail\")\n",
    "\n",
    "        # Find closest station index for each node in chunk\n",
    "        indices = distances.argmin(axis=1)\n",
    "        closest_distances = distances[indices]\n",
    "        closest_station_indices[i:chunk_end] = distances.argmin(axis=1)\n",
    "        pbar.update(chunk_end - i)\n",
    "\n",
    "print(f\"Processed {len(node_ids)} nodes in chunks of {chunk_size}\")\n",
    "print(f\"Example: Node 0 closest to station index {closest_station_indices[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for railway_index in range(railway_stations.shape[0]):\n",
    "    indices = np.argwhere(closest_station_indices == railway_index).ravel()\n",
    "    if indices.size > 0:\n",
    "        closest_sub_index = np.argmin(closest_station_distances[indices])\n",
    "        index = indices[closest_sub_index]\n",
    "    else:\n",
    "        distances = haversine(\n",
    "            node_lons,\n",
    "            node_lats,\n",
    "            station_lons[railway_index],\n",
    "            station_lats[railway_index],\n",
    "        )\n",
    "        index = distances.argmin()\n",
    "    node_id = node_ids[index]\n",
    "    graph.nodes[node_id].setdefault(\"stations\", []).append(\n",
    "        railway_stations.iloc[railway_index].to_dict()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde48f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Awaitable\n",
    "\n",
    "\n",
    "async def get_journey_duration(\n",
    "    tf_client: tfl.api.Tfl,\n",
    "    source: tuple[float, float],\n",
    "    target: tuple[float, float],\n",
    "):\n",
    "    \"\"\"Get minimum journey duration for a node. (lat, long)\"\"\"\n",
    "    try:\n",
    "        results = await tf_client.get_journey_results(\n",
    "            from_location=source,\n",
    "            to_location=target,\n",
    "            arrival_datetime=datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "            + datetime.timedelta(hours=12),\n",
    "            modes=[\n",
    "                tfl.models.ModeId.TUBE,\n",
    "                tfl.models.ModeId.OVERGROUND,\n",
    "                tfl.models.ModeId.ELIZABETH_LINE,\n",
    "                tfl.models.ModeId.DLR,\n",
    "                tfl.models.ModeId.WALKING,\n",
    "            ],\n",
    "            use_multi_modal_call=False,\n",
    "        )\n",
    "        if isinstance(results, tfl.models.DisambiguationResult):\n",
    "            if len(results.to_location_disambiguation.disambiguation_options or []) > 0:\n",
    "                raise ValueError(\"Ambigious to location\")\n",
    "            if results.from_location_disambiguation.disambiguation_options is not None:\n",
    "                disambiguation_options = (\n",
    "                    results.from_location_disambiguation.disambiguation_options\n",
    "                )\n",
    "                best_option = max(disambiguation_options, key=lambda x: x.match_quality)\n",
    "                return await get_journey_duration(\n",
    "                    tf_client, (best_option.place.lat, best_option.place.lon), target\n",
    "                )\n",
    "            raise ValueError(\"Ambigious result\")\n",
    "        min_duration_minutes = min(\n",
    "            sum(leg.duration for leg in journey.legs) for journey in results.journeys\n",
    "        )\n",
    "        return min_duration_minutes\n",
    "    except tfl.exceptions.TflApiError:\n",
    "        logging.exception(f\"API error for node {node_id}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "async def identify[T, R](key: T, awaitable: Awaitable[R]) -> tuple[T, R]:\n",
    "    return key, await awaitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node_id, node_attribute in graph.nodes.items():\n",
    "    node_lat = node_attribute[\"lat\"]\n",
    "    node_lon = node_attribute[\"lon\"]\n",
    "    break\n",
    "node_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd77e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "target = (51.518088819704815, -0.10794336452296352)\n",
    "target_lat, target_lon = target\n",
    "\n",
    "closest_node = None\n",
    "closest_distance = float(\"inf\")\n",
    "for node_id, node_attribute in graph.nodes.items():\n",
    "    node_lat = node_attribute[\"lat\"]\n",
    "    node_lon = node_attribute[\"lon\"]\n",
    "    if round(node_lat, 3) == 51.518 and round(node_lon, 3) == -0.108:\n",
    "        print(node_id)\n",
    "    distance = haversine(node_lon, node_lat, target_lon, target_lat)\n",
    "    if distance < closest_distance:\n",
    "        closest_distance = distance\n",
    "        closest_node = node_id\n",
    "if closest_node is None:\n",
    "    raise ValueError(\"No close node\")\n",
    "\n",
    "frontier: set[str] = {\n",
    "    node_id\n",
    "    for node_id, node_attributes in graph.nodes.items()\n",
    "    if \"stations\" in node_attributes\n",
    "}\n",
    "frontier.add(closest_node)\n",
    "\n",
    "max_duration = 30\n",
    "\n",
    "node_id_durations: dict[str, int | float] = {}\n",
    "frontier_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "if Path(\"trainline_road_node_durations.pkl\").exists():\n",
    "    node_id_durations = pickle.loads(\n",
    "        Path(\"trainline_road_node_durations.pkl\").read_bytes()\n",
    "    )\n",
    "else:\n",
    "    with tqdm.tqdm() as progress_bar:\n",
    "        progress_bar.update(0)\n",
    "        while frontier:\n",
    "            progress_bar.set_description_str(\n",
    "                \" |  \".join(\n",
    "                    (\n",
    "                        f\"{key}: {value}\"\n",
    "                        for key, value in (\n",
    "                            (\"Frontier Size\", len(frontier)),\n",
    "                            (\n",
    "                                \"Min Duration\",\n",
    "                                min(frontier_durations)\n",
    "                                if frontier_durations\n",
    "                                else \"N/A\",\n",
    "                            ),\n",
    "                            (\n",
    "                                \"Mean Duration\",\n",
    "                                round(statistics.mean(frontier_durations))\n",
    "                                if frontier_durations\n",
    "                                else \"N/A\",\n",
    "                            ),\n",
    "                            (\n",
    "                                \"Max Duration\",\n",
    "                                max(frontier_durations)\n",
    "                                if frontier_durations\n",
    "                                else \"N/A\",\n",
    "                            ),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                + \". Iteration\",\n",
    "                refresh=True,\n",
    "            )\n",
    "            awaitables = [\n",
    "                identify(\n",
    "                    node_id,\n",
    "                    get_journey_duration(\n",
    "                        tf_client,\n",
    "                        (graph.nodes[node_id][\"lat\"], graph.nodes[node_id][\"lon\"]),\n",
    "                        target,\n",
    "                    ),\n",
    "                )\n",
    "                for node_id in frontier\n",
    "            ]\n",
    "            next_frontier = set()\n",
    "            frontier_durations.clear()\n",
    "            async for future in asyncio.as_completed(awaitables):\n",
    "                node_id, duration = await future\n",
    "                if duration is not None:\n",
    "                    frontier_durations.append(duration)\n",
    "                    node_id_durations[node_id] = min(\n",
    "                        duration, node_id_durations.get(node_id, float(\"inf\"))\n",
    "                    )\n",
    "                    if duration <= max_duration:\n",
    "                        next_frontier.update(\n",
    "                            adjacent_node_id\n",
    "                            for adjacent_node_id in graph.adj[node_id]\n",
    "                            if adjacent_node_id not in node_id_durations\n",
    "                        )\n",
    "                else:\n",
    "                    node_id_durations[node_id] = float(\"inf\")\n",
    "                progress_bar.set_description_str(\n",
    "                    \" |  \".join(\n",
    "                        (\n",
    "                            f\"{key}: {value}\"\n",
    "                            for key, value in (\n",
    "                                (\"Frontier Size\", len(frontier)),\n",
    "                                (\n",
    "                                    \"Min Duration\",\n",
    "                                    min(frontier_durations)\n",
    "                                    if frontier_durations\n",
    "                                    else \"N/A\",\n",
    "                                ),\n",
    "                                (\n",
    "                                    \"Mean Duration\",\n",
    "                                    round(statistics.mean(frontier_durations))\n",
    "                                    if frontier_durations\n",
    "                                    else \"N/A\",\n",
    "                                ),\n",
    "                                (\n",
    "                                    \"Max Duration\",\n",
    "                                    max(frontier_durations)\n",
    "                                    if frontier_durations\n",
    "                                    else \"N/A\",\n",
    "                                ),\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                    + \". Iteration\",\n",
    "                    refresh=True,\n",
    "                )\n",
    "            frontier = next_frontier\n",
    "            progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864aa42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(node_id_durations)\n",
    "# Processed 91817 nodes in 3hrs 14mins\n",
    "\n",
    "# Path(\"trainline_road_node_durations.pkl\").write_bytes(pickle.dumps(node_id_durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031688fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "subset = {\n",
    "    node_id\n",
    "    for node_id, duration in node_id_durations.items()\n",
    "    if duration <= max_duration\n",
    "}\n",
    "subgraph = graph.subgraph(subset)\n",
    "\n",
    "# Use NetworkX's built-in connected components\n",
    "sub_graphs = list(nx.weakly_connected_components(subgraph))\n",
    "\n",
    "print(f\"Found {len(sub_graphs)} connected components\")\n",
    "print(f\"Largest component has {max(len(sg) for sg in sub_graphs)} nodes\")\n",
    "print(f\"Smallest component has {min(len(sg) for sg in sub_graphs)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e89b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4573b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial\n",
    "\n",
    "for _subgraph in tqdm.tqdm(sub_graphs):\n",
    "    points = np.array(\n",
    "        [(graph.nodes[nid][\"lon\"], graph.nodes[nid][\"lat\"]) for nid in _subgraph]\n",
    "    )\n",
    "    if len(_subgraph) < 3:\n",
    "        continue\n",
    "    hull = scipy.spatial.ConvexHull(points)\n",
    "    plt.plot(points[hull.vertices, 0], points[hull.vertices, 1], \"r--\", lw=2)\n",
    "    plt.plot(points[hull.vertices[0], 0], points[hull.vertices[0], 1], \"ro\")\n",
    "    # Connect the first and last point to close the hull\n",
    "    plt.plot(\n",
    "        [points[hull.vertices[-1], 0], points[hull.vertices[0], 0]],\n",
    "        [points[hull.vertices[-1], 1], points[hull.vertices[0], 1]],\n",
    "        \"r--\",\n",
    "        lw=2,\n",
    "    )\n",
    "all_points = np.array(\n",
    "    [(graph.nodes[nid][\"lon\"], graph.nodes[nid][\"lat\"]) for nid in subset]\n",
    ")\n",
    "plt.scatter(all_points[:, 0], all_points[:, 1], s=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f0bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial\n",
    "\n",
    "_subgraph = max(sub_graphs, key=len)\n",
    "points = np.array(\n",
    "    [(graph.nodes[nid][\"lon\"], graph.nodes[nid][\"lat\"]) for nid in _subgraph]\n",
    ")\n",
    "hull = scipy.spatial.ConvexHull(points)\n",
    "plt.plot(points[hull.vertices, 0], points[hull.vertices, 1], \"r--\", lw=2)\n",
    "plt.plot(points[hull.vertices[0], 0], points[hull.vertices[0], 1], \"ro\")\n",
    "# Connect the first and last point to close the hull\n",
    "plt.plot(\n",
    "    [points[hull.vertices[-1], 0], points[hull.vertices[0], 0]],\n",
    "    [points[hull.vertices[-1], 1], points[hull.vertices[0], 1]],\n",
    "    \"r--\",\n",
    "    lw=2,\n",
    ")\n",
    "all_points = np.array(\n",
    "    [(graph.nodes[nid][\"lon\"], graph.nodes[nid][\"lat\"]) for nid in subset]\n",
    ")\n",
    "plt.scatter(all_points[:, 0], all_points[:, 1], s=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e029e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "if False:\n",
    "    plt.figure()\n",
    "    for subgraph in sub_graphs:\n",
    "        # Get points for this cluster\n",
    "        cluster_points = np.array(\n",
    "            [(graph.nodes[nid][\"lon\"], graph.nodes[nid][\"lat\"]) for nid in subgraph]\n",
    "        )\n",
    "\n",
    "        if len(cluster_points) < 10:  # Skip very small clusters or handle as points\n",
    "            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], color=\"blue\")\n",
    "            continue\n",
    "\n",
    "        # Plot blue points\n",
    "        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], color=\"blue\", s=0.01)\n",
    "\n",
    "        # Compute KDE with adjustable bandwidth (smaller = tighter, more detailed; try 0.01-0.1 for your scale)\n",
    "        kde = gaussian_kde(cluster_points.T, bw_method=0.05)\n",
    "\n",
    "        # Create evaluation grid (higher resolution for smoother contours)\n",
    "        xmin, xmax = (\n",
    "            cluster_points[:, 0].min() - 0.005,\n",
    "            cluster_points[:, 0].max() + 0.005,\n",
    "        )\n",
    "        ymin, ymax = (\n",
    "            cluster_points[:, 1].min() - 0.005,\n",
    "            cluster_points[:, 1].max() + 0.005,\n",
    "        )\n",
    "        xi, yi = np.mgrid[xmin:xmax:200j, ymin:ymax:200j]  # 200x200 grid for detail\n",
    "        coords = np.vstack([xi.ravel(), yi.ravel()])\n",
    "        z = kde.evaluate(coords).reshape(xi.shape)\n",
    "\n",
    "        # Determine contour level to enclose points (tune level_factor: 0.01-0.1 for outer, higher for tighter)\n",
    "        densities_at_points = kde.evaluate(cluster_points.T)\n",
    "        min_density = np.min(densities_at_points)\n",
    "        level = (\n",
    "            min_density * 0.5\n",
    "        )  # Start with 0.5; decrease to loosen, increase to tighten\n",
    "\n",
    "        # Plot red dashed contour\n",
    "        plt.contour(xi, yi, z, levels=[level], colors=\"red\", linestyles=\"dashed\")\n",
    "\n",
    "    # Plot red outliers (assuming you have a list of outlier points)\n",
    "    # outliers = np.array([...])  # Your red points\n",
    "    # plt.scatter(outliers[:, 0], outliers[:, 1], color='red')\n",
    "\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.get_fignums()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# largest_subgraph = max(sub_graphs, key=len)\n",
    "\n",
    "# nx.draw(graph.subgraph(largest_subgraph))\n",
    "\n",
    "plt.hist(\n",
    "    [len(sg) for sg in sub_graphs],\n",
    "    # bins=range(1, max(len(sg) for sg in sub_graphs) + 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ebbe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.connected_components(graph, [graph.nodes[\"-0.043770,51.538328\"], graph.nodes[\"-0.025515,51.527226\"]])\n",
    "subset = {\n",
    "    node_id\n",
    "    for node_id, duration in node_id_durations.items()\n",
    "    if duration <= max_duration\n",
    "}\n",
    "subgraph = graph.subgraph(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node_id, node_attribute in graph.nodes.items():\n",
    "    duration = node_id_durations.get(node_id, float(\"inf\"))\n",
    "    node_attribute[\"duration\"] = duration\n",
    "\n",
    "isochrone_subgraph = nx.ego_graph(\n",
    "    graph, closest_node, radius=max_duration, undirected=True, distance=\"duration\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7128256",
   "metadata": {},
   "outputs": [],
   "source": [
    "isochrone_subgraph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graphs = list(nx.weakly_connected_components(isochrone_subgraph))\n",
    "len(sub_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1558575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial\n",
    "\n",
    "for _subgraph in tqdm.tqdm(sub_graphs):\n",
    "    points = np.array(\n",
    "        [(graph.nodes[nid][\"lon\"], graph.nodes[nid][\"lat\"]) for nid in _subgraph]\n",
    "    )\n",
    "    if len(_subgraph) < 3:\n",
    "        continue\n",
    "    hull = scipy.spatial.ConvexHull(points)\n",
    "    plt.plot(points[hull.vertices, 0], points[hull.vertices, 1], \"r--\", lw=2)\n",
    "    plt.plot(points[hull.vertices[0], 0], points[hull.vertices[0], 1], \"ro\")\n",
    "    # Connect the first and last point to close the hull\n",
    "    plt.plot(\n",
    "        [points[hull.vertices[-1], 0], points[hull.vertices[0], 0]],\n",
    "        [points[hull.vertices[-1], 1], points[hull.vertices[0], 1]],\n",
    "        \"r--\",\n",
    "        lw=2,\n",
    "    )\n",
    "all_points = np.array(\n",
    "    [(graph.nodes[nid][\"lon\"], graph.nodes[nid][\"lat\"]) for nid in subset]\n",
    ")\n",
    "plt.scatter(all_points[:, 0], all_points[:, 1], s=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = nx.spring_layout(isochrone_subgraph)\n",
    "nx.draw(isochrone_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd53030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary_adjacent_nodes(\n",
    "    parent_graph: nx.DiGraph,\n",
    "    subgraph_nodes: set[str],\n",
    ") -> tuple[set[str], set[str]]:\n",
    "    \"\"\"\n",
    "    Get the boundary nodes of the subgraph and their immediate neighbors outside the subgraph.\n",
    "\n",
    "    Returns:\n",
    "        boundary_nodes: Nodes in subgraph that have neighbors outside subgraph\n",
    "        adjacent_nodes: Nodes outside subgraph that are direct neighbors of boundary nodes\n",
    "    \"\"\"\n",
    "    boundary_nodes = set()\n",
    "    adjacent_nodes = set()\n",
    "\n",
    "    for node in subgraph_nodes:\n",
    "        neighbors = set(parent_graph.successors(node)) | set(\n",
    "            parent_graph.predecessors(node)\n",
    "        )\n",
    "        outside_neighbors = neighbors - subgraph_nodes\n",
    "        if outside_neighbors:\n",
    "            boundary_nodes.add(node)\n",
    "            adjacent_nodes.update(outside_neighbors)\n",
    "\n",
    "    return boundary_nodes, adjacent_nodes\n",
    "\n",
    "\n",
    "def get_hull_node_ids(\n",
    "    graph: nx.DiGraph,\n",
    "    subgraph_nodes: set[str],\n",
    ") -> list[str]:\n",
    "    \"\"\"Get the node IDs corresponding to convex hull vertices in order.\"\"\"\n",
    "    points = np.array(\n",
    "        [(graph.nodes[nid][\"lon\"], graph.nodes[nid][\"lat\"]) for nid in subgraph_nodes]\n",
    "    )\n",
    "    node_list = list(subgraph_nodes)\n",
    "\n",
    "    if len(points) < 3:\n",
    "        return node_list\n",
    "\n",
    "    hull = scipy.spatial.ConvexHull(points)\n",
    "    return [node_list[i] for i in hull.vertices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a70d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "\n",
    "# largest_subgraph_nodes = max(sub_graphs, key=len)\n",
    "# hull_nodes = get_hull_node_ids(graph, largest_subgraph_nodes)\n",
    "\n",
    "# _subgraph = graph.subgraph(largest_subgraph_nodes)\n",
    "\n",
    "# start = hull_nodes[0]\n",
    "# end = hull_nodes[1]\n",
    "\n",
    "\n",
    "# def haversine_node(graph: nx.DiGraph, start: str, end: str) -> float:\n",
    "#     try:\n",
    "#         length = nx.shortest_path_length(_subgraph, start, end)\n",
    "#     except nx.NetworkXNoPath:\n",
    "#         try:\n",
    "#             length = nx.shortest_path_length(graph, start, end)\n",
    "#         except nx.NetworkXNoPath:\n",
    "#             return float(\"inf\")\n",
    "#     return length\n",
    "\n",
    "\n",
    "# best_length = float(\"inf\")\n",
    "# best = None\n",
    "# with tqdm.tqdm(largest_subgraph_nodes) as pbar:\n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         for node_id, length in executor.map(\n",
    "#             lambda nid: (nid, haversine_node(graph, start, nid)),\n",
    "#             largest_subgraph_nodes,\n",
    "#         ):\n",
    "#             if length < best_length:\n",
    "#                 best_length = length\n",
    "#                 best = node_id\n",
    "#             pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from shapely.geometry import LineString, Point, Polygon\n",
    "\n",
    "# def make_iso_polys(G, edge_buff=25, node_buff=50, infill=False):\n",
    "multi_graph = nx.MultiDiGraph(graph)\n",
    "# WGS84 coordinate reference system which is written as EPSG:4326\n",
    "multi_graph.graph[\"crs\"] = \"EPSG:4326\"\n",
    "for node_id in multi_graph.nodes:\n",
    "    multi_graph.nodes[node_id][\"x\"] = multi_graph.nodes[node_id][\"lon\"]\n",
    "    multi_graph.nodes[node_id][\"y\"] = multi_graph.nodes[node_id][\"lat\"]\n",
    "\n",
    "\n",
    "G = multi_graph\n",
    "edge_buff = 25\n",
    "node_buff = 0.1\n",
    "infill = True\n",
    "\n",
    "\n",
    "isochrone_polys = []\n",
    "\n",
    "new_graph = nx.Graph()\n",
    "new_graph.add_node(\n",
    "    closest_node, x=G.nodes[closest_node][\"lon\"], y=G.nodes[closest_node][\"lat\"]\n",
    ")\n",
    "for node_id in G.nodes:\n",
    "    duration = node_id_durations.get(node_id, float(\"inf\"))\n",
    "    new_graph.add_node(node_id, x=G.nodes[node_id][\"lon\"], y=G.nodes[node_id][\"lat\"])\n",
    "    new_graph.add_edge(closest_node, node_id, duration=duration)\n",
    "\n",
    "nG = nx.MultiGraph(new_graph)\n",
    "nG.graph[\"crs\"] = \"EPSG:4326\"\n",
    "\n",
    "tmp_subgraph = nx.ego_graph(\n",
    "    nG, closest_node, radius=max_duration, undirected=False, distance=\"duration\"\n",
    ")\n",
    "\n",
    "subgraph = nx.subgraph(\n",
    "    G,\n",
    "    [\n",
    "        node\n",
    "        for node, data in tmp_subgraph.nodes(data=True)\n",
    "        # if data.get(\"duration\", float(\"inf\")) <= max_duration\n",
    "    ],\n",
    ")\n",
    "\n",
    "node_points = [\n",
    "    Point((data[\"x\"], data[\"y\"])) for node, data in subgraph.nodes(data=True)\n",
    "]\n",
    "nodes_gdf = gpd.GeoDataFrame({\"id\": list(subgraph.nodes)}, geometry=node_points)\n",
    "nodes_gdf = nodes_gdf.set_index(\"id\")\n",
    "\n",
    "edge_lines = []\n",
    "for n_fr, n_to in subgraph.edges():\n",
    "    f = nodes_gdf.loc[n_fr].geometry\n",
    "    t = nodes_gdf.loc[n_to].geometry\n",
    "    edge_lookup = G.get_edge_data(n_fr, n_to)[0].get(\"geometry\", LineString([f, t]))\n",
    "    edge_lines.append(edge_lookup)\n",
    "\n",
    "# TODO?\n",
    "# n = nodes_gdf.buffer(node_buff).geometry\n",
    "# e = gpd.GeoSeries(edge_lines).buffer(edge_buff).geometry\n",
    "# all_gs = list(n) + list(e)\n",
    "# new_iso = gpd.GeoSeries(all_gs).union_all()\n",
    "new_iso = gpd.GeoSeries(\n",
    "    list(nodes_gdf.geometry) + list(gpd.GeoSeries(edge_lines).geometry)\n",
    ").union_all()\n",
    "\n",
    "# try to fill in surrounded areas so shapes will appear solid and\n",
    "# blocks without white space inside them\n",
    "if infill:\n",
    "    new_iso = Polygon(new_iso.exterior)\n",
    "    isochrone_polys.append(new_iso)\n",
    "else:\n",
    "    isochrone_polys.append(new_iso)\n",
    "\n",
    "\n",
    "# make the isochrone polygons\n",
    "# TODO: Why does node_buff == 0 cause empty polygons on `n = nodes_gdf.buffer(node_buff).geometry`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_iso.buffer(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad03c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(geometry=[new_iso])\n",
    "import osmnx as ox\n",
    "\n",
    "iso_colors = ox.plot.get_colors(n=1, cmap=\"plasma\", start=0)\n",
    "# # plot the network then add isochrones as colored polygon patches\n",
    "fig, ax = ox.plot.plot_graph(\n",
    "    multi_graph,\n",
    "    show=False,\n",
    "    close=False,\n",
    "    edge_color=\"#999999\",\n",
    "    edge_alpha=0.2,\n",
    "    node_size=0,\n",
    "    figsize=(16, 16),\n",
    ")\n",
    "gdf.plot(ax=ax, color=iso_colors, ec=\"none\", alpha=0.6, zorder=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bbf540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_iso_polys(G, subgraph):\n",
    "    node_points = [\n",
    "        Point((data[\"x\"], data[\"y\"])) for node, data in subgraph.nodes(data=True)\n",
    "    ]\n",
    "    nodes_gdf = gpd.GeoDataFrame({\"id\": list(subgraph.nodes)}, geometry=node_points)\n",
    "    nodes_gdf = nodes_gdf.set_index(\"id\")\n",
    "\n",
    "    edge_lines = []\n",
    "    for n_fr, n_to in subgraph.edges():\n",
    "        f = nodes_gdf.loc[n_fr].geometry\n",
    "        t = nodes_gdf.loc[n_to].geometry\n",
    "        edge_lookup = G.get_edge_data(n_fr, n_to).get(\"geometry\", LineString([f, t]))\n",
    "        edge_lines.append(edge_lookup)\n",
    "\n",
    "    return gpd.GeoSeries(\n",
    "        list(nodes_gdf.buffer(0).geometry)\n",
    "        + list(gpd.GeoSeries(edge_lines).buffer(25).geometry)\n",
    "    ).union_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82987a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf.iloc[0].geometry.is_closed\n",
    "# gdf.iloc[0].geometry.segmentize()\n",
    "\n",
    "for node_id in graph.nodes:\n",
    "    graph.nodes[node_id][\"x\"] = graph.nodes[node_id][\"lon\"]\n",
    "    graph.nodes[node_id][\"y\"] = graph.nodes[node_id][\"lat\"]\n",
    "\n",
    "separated_subgraphs = list(nx.weakly_connected_components(subgraph))\n",
    "\n",
    "# nx.draw(graph.subgraph(separated_subgraphs[2]))\n",
    "iso = make_iso_polys(graph, graph.subgraph(separated_subgraphs[0]))\n",
    "iso.exterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f1343",
   "metadata": {},
   "outputs": [],
   "source": [
    "roads_gdf = gpd.read_file(\n",
    "    \"/home/cemlyn/Downloads/greater-london-251126-free.shp/gis_osm_roads_free_1.shp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roads_gdf.union_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e4f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12921495",
   "metadata": {},
   "outputs": [],
   "source": [
    "_sub_graph = graph.subgraph(separated_subgraphs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f9d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roads_gdf.geometry.contains()\n",
    "\n",
    "for node_attribute in _sub_graph.nodes.values():\n",
    "    point = Point((node_attribute[\"lon\"], node_attribute[\"lat\"]))\n",
    "    containing_roads = roads_gdf[roads_gdf.geometry.contains(point)]\n",
    "    if not containing_roads.empty:\n",
    "        print(containing_roads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flathunt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
