{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41de398",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c475e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import concurrent.futures\n",
    "import itertools\n",
    "import os\n",
    "import webbrowser\n",
    "from datetime import time, timezone\n",
    "\n",
    "import dotenv\n",
    "import geopandas as gpd\n",
    "import httpx\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import shapely\n",
    "import shapely.plotting\n",
    "import tqdm\n",
    "import tqdm.asyncio\n",
    "from matplotlib.patches import PathPatch\n",
    "from matplotlib.path import Path\n",
    "from shapely.geometry import LineString, Point\n",
    "\n",
    "import rightmove\n",
    "import tfl.api\n",
    "import tfl.exceptions\n",
    "import tfl.models\n",
    "from tfl.api import get_next_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_MODES = [\n",
    "    tfl.models.ModeId.TUBE,\n",
    "    tfl.models.ModeId.OVERGROUND,\n",
    "    tfl.models.ModeId.DLR,\n",
    "    tfl.models.ModeId.ELIZABETH_LINE,\n",
    "    tfl.models.ModeId.WALKING,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc23ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_client = tfl.api.Tfl(app_key=os.environ[\"FLATHUNT__TFL_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa00f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_facilities = await tf_client.get_stations_facilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = await tf_client.get_all_lines_routes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f23062",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_id_stop_points: dict[str, list[tfl.models.StopPointDetail]] = {}\n",
    "for line in tqdm.tqdm(lines):\n",
    "    if line.mode_name not in ALLOWED_MODES:\n",
    "        continue\n",
    "    line_id_stop_points[line.id] = await tf_client.get_stop_points_by_line(line.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_station_name: dict[str, str] = {}\n",
    "for stop_point_list in line_id_stop_points.values():\n",
    "    for stop_point in stop_point_list:\n",
    "        if stop_point.id in id_to_station_name:\n",
    "            assert id_to_station_name[stop_point.id] == stop_point.common_name, (\n",
    "                f\"Conflict for {stop_point.id}: {id_to_station_name[stop_point.id]} vs {stop_point.common_name}\"\n",
    "            )\n",
    "        id_to_station_name[stop_point.id] = stop_point.common_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_station_durations: dict[str, dict[str, dict[str, float]]] = {}\n",
    "\n",
    "TARGET_DATETIME = get_next_datetime(time(9, 0, 0, tzinfo=timezone.utc))\n",
    "# https://techforum.tfl.gov.uk/t/timetable-between-two-stations-on-elizabeth-line/2251/2\n",
    "# Unfortunately, the Journey API seems to struggle with some Elizabeth Line queries,\n",
    "#  and likewise for the Overground/National Rail interchange stations.\n",
    "if TARGET_DATETIME is None:\n",
    "    line_id_stop_point_timetables: dict[\n",
    "        str, dict[str, dict[tfl.api.Direction, tfl.models.TimetableResponse]]\n",
    "    ] = {}\n",
    "\n",
    "    async def do_work(line_id_stop_point_timetables, line_id, stop_points):\n",
    "        for stop_point in stop_points:\n",
    "            try:\n",
    "                result = await tf_client.get_timetable(\n",
    "                    line_id, stop_point.naptan_id, None\n",
    "                )\n",
    "                if result.disambiguation is None:\n",
    "                    line_id_stop_point_timetables.setdefault(line_id, {}).setdefault(\n",
    "                        stop_point.naptan_id, {}\n",
    "                    )[tfl.api.Direction(result.direction)] = result\n",
    "                else:\n",
    "                    for direction in tfl.api.Direction:\n",
    "                        result = await tf_client.get_timetable(\n",
    "                            line_id, stop_point.naptan_id, direction\n",
    "                        )\n",
    "                        line_id_stop_point_timetables.setdefault(\n",
    "                            line_id, {}\n",
    "                        ).setdefault(stop_point.naptan_id, {})[direction] = result\n",
    "            except tfl.exceptions.TflApiError as e:\n",
    "                # 404: Stop not found, 400: Direction not found (common for National Rail)\n",
    "                if e.http_status_code in (400, 404):\n",
    "                    continue\n",
    "                raise\n",
    "            except httpx.HTTPStatusError as e:\n",
    "                # Catch raw HTTP errors not wrapped by TflApiError (e.g., 400 for tram/national rail)\n",
    "                if e.response.status_code == 400:\n",
    "                    continue\n",
    "                raise\n",
    "\n",
    "    async for future in tqdm.asyncio.tqdm(\n",
    "        asyncio.as_completed(\n",
    "            [\n",
    "                do_work(line_id_stop_point_timetables, line_id, stop_points)\n",
    "                for line_id, stop_points in line_id_stop_points.items()\n",
    "            ]\n",
    "        ),\n",
    "        total=len(line_id_stop_points),\n",
    "    ):\n",
    "        await future\n",
    "\n",
    "    # Build a complete lookup of travel times between all station pairs using station_intervals\n",
    "    # Structure: line_id -> from_station_id -> to_station_id -> duration (minutes)\n",
    "\n",
    "    for line_id, stop_timetables in tqdm.tqdm(line_id_stop_point_timetables.items()):\n",
    "        all_station_durations[line_id] = {}\n",
    "\n",
    "        for naptan_id, direction_timetables in stop_timetables.items():\n",
    "            for direction, timetable_response in direction_timetables.items():\n",
    "                if timetable_response is None or timetable_response.timetable is None:\n",
    "                    continue\n",
    "                if not timetable_response.timetable.routes:\n",
    "                    continue\n",
    "\n",
    "                # Get the departure station ID from the timetable\n",
    "                from_station_id = timetable_response.timetable.departure_stop_id\n",
    "\n",
    "                for route in timetable_response.timetable.routes:\n",
    "                    if not route.station_intervals:\n",
    "                        continue\n",
    "\n",
    "                    # Use the first station_interval (they're typically similar)\n",
    "                    station_interval = route.station_intervals[0]\n",
    "\n",
    "                    if from_station_id not in all_station_durations[line_id]:\n",
    "                        all_station_durations[line_id][from_station_id] = {}\n",
    "\n",
    "                    for interval in station_interval.intervals:\n",
    "                        to_station_id = interval.stop_id\n",
    "                        duration = interval.time_to_arrival\n",
    "\n",
    "                        # Keep the value (or update if we find a different one - they should match)\n",
    "                        if (\n",
    "                            to_station_id\n",
    "                            not in all_station_durations[line_id][from_station_id]\n",
    "                        ):\n",
    "                            all_station_durations[line_id][from_station_id][\n",
    "                                to_station_id\n",
    "                            ] = duration\n",
    "\n",
    "    print(\"Summary of station duration data:\")\n",
    "    for line_id, from_stations in all_station_durations.items():\n",
    "        total_pairs = sum(len(to_stations) for to_stations in from_stations.values())\n",
    "        print(\n",
    "            f\"  {line_id}: {len(from_stations)} departure stations, {total_pairs} total pairs\"\n",
    "        )\n",
    "else:\n",
    "    queries = []\n",
    "    for line_id, stop_points in line_id_stop_points.items():\n",
    "        for stop_point, other_stop_point in itertools.combinations(stop_points, 2):\n",
    "            queries.append((line_id, stop_point, other_stop_point))\n",
    "\n",
    "    async def process_query_queue(line_id, stop_point, other_stop_point):\n",
    "        try:\n",
    "            journey_results = await tf_client.get_journey_results(\n",
    "                from_location=stop_point.id,\n",
    "                to_location=other_stop_point.id,\n",
    "                arrival_datetime=TARGET_DATETIME,\n",
    "                modes=ALLOWED_MODES,\n",
    "                use_multi_modal_call=False,\n",
    "            )\n",
    "        except tfl.exceptions.JourneyNotFoundError:\n",
    "            return line_id, stop_point.id, other_stop_point.id, None\n",
    "        if not isinstance(journey_results, tfl.models.JourneyResults):\n",
    "            return line_id, stop_point.id, other_stop_point.id, None\n",
    "        min_duration = min(jr.duration for jr in journey_results.journeys)\n",
    "        return line_id, stop_point.id, other_stop_point.id, min_duration\n",
    "\n",
    "    awaitables = [\n",
    "        process_query_queue(line_id, stop_point, other_stop_point)\n",
    "        for line_id, stop_point, other_stop_point in queries\n",
    "        if (\n",
    "            all_station_durations.get(line_id, {})\n",
    "            .get(stop_point.id, {})\n",
    "            .get(other_stop_point.id)\n",
    "            is None\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for future in tqdm.asyncio.tqdm(\n",
    "        asyncio.as_completed(awaitables), total=len(awaitables)\n",
    "    ):\n",
    "        line_id, from_station_id, to_station_id, min_duration = await future\n",
    "        if min_duration is not None:\n",
    "            all_station_durations.setdefault(line_id, {}).setdefault(\n",
    "                from_station_id, {}\n",
    "            )[to_station_id] = min_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ccf520",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    id_to_station_name[src_id]: {\n",
    "        id_to_station_name[dst_id]: duration for dst_id, duration in dsts.items()\n",
    "    }\n",
    "    for src_id, dsts in all_station_durations[\"circle\"].items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fff77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "roads_gdf = gpd.read_file(\n",
    "    # /Users/cemlyn/Downloads/greater-london-251126-free/gis_osm_buildings_a_free_1.cpg\n",
    "    \"/Users/cemlyn/Downloads/greater-london-251126-free/gis_osm_roads_free_1.shp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce85d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roads_gdf = roads_gdf.to_crs(\"EPSG:27700\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b169a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_to_meters(lon: float, lat: float):\n",
    "    point_wgs84 = gpd.GeoSeries([Point(lon, lat)], crs=\"EPSG:4326\")\n",
    "    point_osgb36 = point_wgs84.to_crs(\"EPSG:27700\")\n",
    "    return point_osgb36.x.item(), point_osgb36.y.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e184eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(x1, y1, x2, y2):\n",
    "    return np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.Graph()\n",
    "for _, road in tqdm.tqdm(roads_gdf.iterrows(), total=len(roads_gdf)):\n",
    "    for i, ((x1, y1), (x2, y2)) in enumerate(itertools.pairwise(road.geometry.coords)):\n",
    "        if (x1, y1) not in graph:\n",
    "            graph.add_node((x1, y1), x=x1, y=y1)\n",
    "        if (x2, y2) not in graph:\n",
    "            graph.add_node((x2, y2), x=x2, y=y2)\n",
    "        if not graph.has_edge((x1, y1), (x2, y2)):\n",
    "            graph.add_edge(\n",
    "                (x1, y1),\n",
    "                (x2, y2),\n",
    "                length=(euclidean(x1, y1, x2, y2)).item(),\n",
    "                geometry=LineString([(x1, y1), (x2, y2)]),\n",
    "            )  # in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef36ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_graph = nx.Graph()\n",
    "missing_pairs = []\n",
    "\n",
    "for line_id in line_id_stop_points.keys():\n",
    "    for stop_point in line_id_stop_points[line_id]:\n",
    "        x, y = project_to_meters(stop_point.lon, stop_point.lat)\n",
    "        if (x, y) not in transport_graph:\n",
    "            transport_graph.add_node(\n",
    "                (x, y),\n",
    "                x=x,\n",
    "                y=y,\n",
    "                station_name=stop_point.common_name,\n",
    "            )\n",
    "\n",
    "    line_durations = all_station_durations.get(line_id, {})\n",
    "\n",
    "    for stop_point, other_stop_point in itertools.combinations(\n",
    "        line_id_stop_points[line_id], 2\n",
    "    ):\n",
    "        # Use naptan_id to match the keys in all_station_durations (from departure_stop_id)\n",
    "        stop_id = stop_point.naptan_id\n",
    "        other_id = other_stop_point.naptan_id\n",
    "\n",
    "        x1, y1 = project_to_meters(stop_point.lon, stop_point.lat)\n",
    "        x2, y2 = project_to_meters(other_stop_point.lon, other_stop_point.lat)\n",
    "\n",
    "        # Try both directions since station_intervals only go one way\n",
    "        time = None\n",
    "        if stop_id in line_durations and other_id in line_durations[stop_id]:\n",
    "            time = line_durations[stop_id][other_id]\n",
    "        elif other_id in line_durations and stop_id in line_durations[other_id]:\n",
    "            time = line_durations[other_id][stop_id]\n",
    "\n",
    "        if time is None:\n",
    "            missing_pairs.append((line_id, stop_id, other_id))\n",
    "            continue\n",
    "\n",
    "        time += 5  # add 5 minutes for boarding/alighting\n",
    "\n",
    "        transport_graph.add_edge(\n",
    "            (x1, y1),\n",
    "            (x2, y2),\n",
    "            time=time,\n",
    "            geometry=LineString(\n",
    "                [\n",
    "                    (x1, y1),\n",
    "                    (x2, y2),\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "print(f\"Missing pairs: {len(missing_pairs)}\")\n",
    "if missing_pairs:\n",
    "    # Show a sample of missing pairs by line\n",
    "    from collections import Counter\n",
    "\n",
    "    line_counts = Counter(line_id for line_id, _, _ in missing_pairs)\n",
    "    print(\"Missing pairs by line:\")\n",
    "    for line_id, count in line_counts.most_common():\n",
    "        print(f\"  {line_id}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34dc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_graph = nx.compose_all([graph, transport_graph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddeda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_node(x, y):\n",
    "    \"\"\"Find the nearest node to a given (x, y) coordinate.\"\"\"\n",
    "    distances = euclidean(x, y, points[:, 0], points[:, 1])\n",
    "    return distances.argmin(axis=0).item()\n",
    "\n",
    "\n",
    "non_transport_nodes = list(graph.nodes)\n",
    "points = np.array([(data[\"x\"], data[\"y\"]) for _, data in graph.nodes(data=True)])\n",
    "\n",
    "for transport_node_key in tqdm.tqdm(transport_graph.nodes):\n",
    "    x = transport_graph.nodes[transport_node_key][\"x\"]\n",
    "    y = transport_graph.nodes[transport_node_key][\"y\"]\n",
    "    closest = find_nearest_node(x, y)\n",
    "    non_transport_key = non_transport_nodes[closest]\n",
    "    whole_graph.add_edge(\n",
    "        transport_node_key,\n",
    "        non_transport_key,\n",
    "        length=(\n",
    "            euclidean(\n",
    "                x,\n",
    "                y,\n",
    "                graph.nodes[non_transport_key][\"x\"],\n",
    "                graph.nodes[non_transport_key][\"y\"],\n",
    "            ).item()\n",
    "        ),\n",
    "        geometry=LineString(\n",
    "            [\n",
    "                (x, y),\n",
    "                (\n",
    "                    graph.nodes[non_transport_key][\"x\"],\n",
    "                    graph.nodes[non_transport_key][\"y\"],\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9624236",
   "metadata": {},
   "outputs": [],
   "source": [
    "meters_per_minute = 60\n",
    "for a, b, data in whole_graph.edges(data=True):\n",
    "    if (\n",
    "        \"station_name\" in whole_graph.nodes[a]\n",
    "        and \"station_name\" in whole_graph.nodes[b]\n",
    "    ):\n",
    "        print(\n",
    "            whole_graph.nodes[a][\"station_name\"],\n",
    "            \"-\",\n",
    "            whole_graph.nodes[b][\"station_name\"],\n",
    "        )\n",
    "        if \"time\" not in data:\n",
    "            raise ValueError\n",
    "    else:\n",
    "        data[\"time\"] = data[\"length\"] / meters_per_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05db56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isochrones(G, node, trip_time: float):\n",
    "    subgraph = nx.ego_graph(G, node, radius=trip_time, distance=\"time\")\n",
    "\n",
    "    remove_edges = set()\n",
    "    for n_fr, n_to in subgraph.edges():\n",
    "        if (\n",
    "            \"station_name\" in subgraph.nodes[n_fr]\n",
    "            and \"station_name\" in subgraph.nodes[n_to]\n",
    "        ):\n",
    "            remove_edges.add((n_fr, n_to))\n",
    "\n",
    "    for n_fr, n_to in remove_edges:\n",
    "        subgraph.remove_edge(n_fr, n_to)\n",
    "\n",
    "    subgraphs_nodes = nx.connected_components(subgraph)\n",
    "\n",
    "    return [nx.subgraph(graph, nodes) for nodes in subgraphs_nodes]\n",
    "\n",
    "\n",
    "def make_poly(G, edge_buff: float, node_buff: float):\n",
    "    node_points = [Point((data[\"x\"], data[\"y\"])) for node, data in G.nodes(data=True)]\n",
    "    nodes_gdf = gpd.GeoDataFrame({\"id\": list(G.nodes)}, geometry=node_points)\n",
    "    nodes_gdf = nodes_gdf.set_index(\"id\")\n",
    "    edge_lines = []\n",
    "    for n_fr, n_to in G.edges():\n",
    "        if \"station_name\" in G.nodes[n_fr] and \"station_name\" in G.nodes[n_to]:\n",
    "            continue\n",
    "        edge_lookup = G.get_edge_data(n_fr, n_to)[\"geometry\"]\n",
    "        edge_lines.append(edge_lookup)\n",
    "    n = nodes_gdf.buffer(node_buff).geometry\n",
    "    e = gpd.GeoSeries(edge_lines).buffer(edge_buff).geometry\n",
    "    all_gs = list(n) + list(e)\n",
    "    new_iso = gpd.GeoSeries(all_gs).union_all()\n",
    "    return new_iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b91eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_iso_subgraphs = []\n",
    "queries = [\n",
    "    (-0.10813726002192411, 51.51804484802881),\n",
    "    (-0.0207016567503272, 51.503329567778614),\n",
    "]\n",
    "MAX_DURATION_MINUTES = 29\n",
    "for query in queries:\n",
    "    x, y = project_to_meters(query[0], query[1])\n",
    "    closest_node_index = find_nearest_node(x, y)\n",
    "    locked_query = non_transport_nodes[closest_node_index]\n",
    "    query_iso_subgraphs.append(\n",
    "        isochrones(whole_graph, locked_query, MAX_DURATION_MINUTES)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_BUFFER = 0\n",
    "EDGE_BUFFER = 25\n",
    "\n",
    "all_polys = []\n",
    "for subgraphs in query_iso_subgraphs:\n",
    "    subgraph_polys = []\n",
    "    with tqdm.tqdm(total=len(subgraphs)) as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for poly in executor.map(\n",
    "                lambda sg: make_poly(sg, NODE_BUFFER, EDGE_BUFFER), subgraphs\n",
    "            ):\n",
    "                subgraph_polys.append(poly)\n",
    "                pbar.update(1)\n",
    "    all_polys.append(subgraph_polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7916c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "a_subgraphs, b_subgraphs = query_iso_subgraphs\n",
    "a_polys, b_polys = all_polys\n",
    "for a_subgraph, a_poly in tqdm.tqdm(\n",
    "    zip(a_subgraphs, a_polys, strict=True), total=len(a_subgraphs)\n",
    "):\n",
    "    for b_subgraph, b_poly in zip(b_subgraphs, b_polys, strict=True):\n",
    "        a_boundary = a_poly.boundary\n",
    "        b_boundary = b_poly.boundary\n",
    "        if (\n",
    "            a_boundary is not None\n",
    "            and b_boundary is not None\n",
    "            and a_boundary.intersects(b_boundary)\n",
    "        ):\n",
    "            pairs.append((a_subgraph, b_subgraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compatible_intersections = []\n",
    "with tqdm.tqdm(total=len(pairs)) as pbar:\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        for intersection in executor.map(\n",
    "            nx.intersection, [a for a, b in pairs], [b for a, b in pairs]\n",
    "        ):\n",
    "            if intersection.number_of_nodes() > 0:\n",
    "                intersection_subgraphs = list(nx.connected_components(intersection))\n",
    "                compatible_intersections.extend(\n",
    "                    [\n",
    "                        nx.subgraph(intersection, nodes)\n",
    "                        for nodes in intersection_subgraphs\n",
    "                    ]\n",
    "                )\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compatible_intersections = [g.copy() for g in compatible_intersections]\n",
    "for intersection in compatible_intersections:\n",
    "    for node_id, node_attributes in intersection.nodes.items():\n",
    "        # TODO: Why is this happening??\n",
    "        node_attributes.update(whole_graph.nodes[node_id])\n",
    "        # Add back the edges\n",
    "        for neighbor, edge_attributes in whole_graph[node_id].items():\n",
    "            if neighbor in intersection.nodes:\n",
    "                intersection.add_edge(node_id, neighbor, **edge_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_polys = []\n",
    "all_graphs = query_iso_subgraphs + [compatible_intersections]\n",
    "with tqdm.tqdm(total=sum(map(len, all_graphs))) as pbar:\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        maps = [\n",
    "            executor.map(lambda sg: make_poly(sg, EDGE_BUFFER, NODE_BUFFER), subgraphs)\n",
    "            for subgraphs in all_graphs\n",
    "        ]\n",
    "        for _map in maps:\n",
    "            subgraph_polys = []\n",
    "            for subgraph in _map:\n",
    "                subgraph_polys.append(subgraph)\n",
    "                pbar.update(1)\n",
    "            all_polys.append(subgraph_polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = []\n",
    "for polys, color, zorder in tqdm.tqdm(\n",
    "    zip(\n",
    "        # [[poly.exterior for poly in ps if not poly.is_empty] for ps in all_polys],\n",
    "        [[poly for poly in ps if not poly.is_empty] for ps in all_polys],\n",
    "        [\"blue\", \"red\", \"cyan\"],\n",
    "        [0, 0, 1],\n",
    "        strict=True,\n",
    "    ),\n",
    "    total=len(all_polys),\n",
    "):\n",
    "    _poly = shapely.union_all(polys)\n",
    "    if isinstance(_poly, shapely.MultiPolygon):\n",
    "        patch = shapely.plotting.patch_from_polygon(\n",
    "            _poly,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            linewidth=0.1,\n",
    "            alpha=0.5 if zorder != 1 else 1.0,\n",
    "            zorder=zorder,\n",
    "        )\n",
    "    elif isinstance(_poly, shapely.MultiLineString):\n",
    "        path = Path.make_compound_path(\n",
    "            *[Path(np.asarray(mline.coords)[:, :2]) for mline in _poly.geoms]\n",
    "        )\n",
    "        patch = PathPatch(\n",
    "            path,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            linewidth=0.1,\n",
    "            alpha=0.5 if zorder != 1 else 1.0,\n",
    "            zorder=zorder,\n",
    "        )\n",
    "    elif isinstance(_poly, shapely.LineString):\n",
    "        path = Path(np.asarray(_poly.coords)[:, :2])\n",
    "        patch = PathPatch(\n",
    "            path,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            linewidth=0.1,\n",
    "            alpha=0.5 if zorder != 1 else 1.0,\n",
    "            zorder=zorder,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected geometry type: {_poly.geom_type}\")\n",
    "    patches.append(patch)\n",
    "figure = plt.figure(dpi=300)\n",
    "ax = figure.gca()\n",
    "roads_gdf.geometry.plot(ax=ax, color=\"black\", linewidth=0.1)\n",
    "for p in patches:\n",
    "    ax.add_patch(p)\n",
    "ax.autoscale_view()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752badf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections = all_polys[-1]\n",
    "check_coords = []\n",
    "for poly, poly_network in zip(intersections, compatible_intersections, strict=True):\n",
    "    if poly.is_empty:\n",
    "        continue\n",
    "    x, y = poly.centroid.x, poly.centroid.y\n",
    "    for node_id, node_attributes in poly_network.nodes(data=True):\n",
    "        if \"station_name\" in node_attributes:\n",
    "            print(f\"Station in intersection: {node_attributes['station_name']}\")\n",
    "            x = node_attributes[\"x\"]\n",
    "            y = node_attributes[\"y\"]\n",
    "    lon, lat = (\n",
    "        gpd.GeoSeries([Point(x, y)], crs=\"EPSG:27700\")\n",
    "        .to_crs(\"EPSG:4326\")\n",
    "        .geometry[0]\n",
    "        .coords[0]\n",
    "    )\n",
    "    check_coords.append((lon, lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4dd220",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_times = {}\n",
    "\n",
    "\n",
    "async def fetch_journey_results(lon, lat, query_lon, query_lat, i):\n",
    "    try:\n",
    "        journey_results = await tf_client.get_journey_results(\n",
    "            from_location=(lat, lon),\n",
    "            to_location=(query_lat, query_lon),\n",
    "            arrival_datetime=TARGET_DATETIME,\n",
    "            modes=[\n",
    "                tfl.models.ModeId.TUBE,\n",
    "                tfl.models.ModeId.OVERGROUND,\n",
    "                tfl.models.ModeId.DLR,\n",
    "                tfl.models.ModeId.ELIZABETH_LINE,\n",
    "                tfl.models.ModeId.WALKING,\n",
    "            ],\n",
    "            use_multi_modal_call=False,\n",
    "        )\n",
    "        if isinstance(journey_results, tfl.models.DisambiguationResult):\n",
    "            print(f\"  Query {(lon, lat)} {i + 1}: Disambiguation result, skipping\")\n",
    "            return None, None\n",
    "        min_time = min(journey.duration for journey in journey_results.journeys)\n",
    "        return (lon, lat), (query_lon, query_lat, min_time)\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error fetching journey results for {(lon, lat)} to {(query_lon, query_lat)}: {e}\"\n",
    "        )\n",
    "        return None, None\n",
    "\n",
    "\n",
    "tasks = []\n",
    "for lon, lat in tqdm.tqdm(check_coords):\n",
    "    for i, (query_lon, query_lat) in enumerate(queries):\n",
    "        tasks.append(fetch_journey_results(lon, lat, query_lon, query_lat, i))\n",
    "\n",
    "results = await asyncio.gather(*tasks)\n",
    "\n",
    "for result in results:\n",
    "    if result[0] is not None and result[1] is not None:\n",
    "        (lon, lat), (query_lon, query_lat, min_time) = result\n",
    "        min_times.setdefault((lon, lat), {})[(query_lon, query_lat)] = min_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8809f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(itertools.chain.from_iterable((times.values() for times in min_times.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce20e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_simplify_tolerance(polygon, max_coords=1000, tol=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Find the minimum tolerance that simplifies a polygon to have fewer than max_coords coordinates.\n",
    "    Uses binary search to find the smallest tolerance that achieves the target.\n",
    "\n",
    "    Args:\n",
    "        polygon: A shapely Polygon\n",
    "        max_coords: Maximum number of coordinates allowed (default 1000)\n",
    "        tol: Convergence tolerance for binary search (default 1e-6)\n",
    "        max_iter: Maximum iterations to prevent infinite loops\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (simplified_exterior, tolerance_used)\n",
    "    \"\"\"\n",
    "    exterior = polygon.exterior\n",
    "    original_coords = len(list(exterior.coords))\n",
    "\n",
    "    # If already under the limit, no simplification needed\n",
    "    if original_coords < max_coords:\n",
    "        return exterior, 0.0\n",
    "\n",
    "    # Find an upper bound that definitely works\n",
    "    # Start with a reasonable guess and double until we get under max_coords\n",
    "    high = 1.0\n",
    "    while len(list(exterior.simplify(high).coords)) >= max_coords:\n",
    "        high *= 2\n",
    "        if high > 1e6:  # Safety limit\n",
    "            raise ValueError(f\"Could not simplify polygon below {max_coords} coords\")\n",
    "\n",
    "    # Binary search for minimum tolerance\n",
    "    low = 0.0\n",
    "    best_exterior = exterior.simplify(high)\n",
    "    best_tolerance = high\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        if high - low < tol:\n",
    "            break\n",
    "\n",
    "        mid = (low + high) / 2\n",
    "        simplified = exterior.simplify(mid)\n",
    "        num_coords = len(list(simplified.coords))\n",
    "\n",
    "        if num_coords < max_coords:\n",
    "            # This tolerance works, try to find a smaller one\n",
    "            best_exterior = simplified\n",
    "            best_tolerance = mid\n",
    "            high = mid\n",
    "        else:\n",
    "            # Need more simplification (higher tolerance)\n",
    "            low = mid\n",
    "\n",
    "    return best_exterior, best_tolerance\n",
    "\n",
    "\n",
    "best_coords = []\n",
    "for poly in intersections:\n",
    "    if poly.is_empty:\n",
    "        continue\n",
    "\n",
    "    exterior, tolerance = find_min_simplify_tolerance(poly, max_coords=1000)\n",
    "\n",
    "    meters = list(exterior.coords)\n",
    "    coords = []\n",
    "    for x, y in meters:\n",
    "        lon, lat = (\n",
    "            gpd.GeoSeries([Point(x, y)], crs=\"EPSG:27700\")\n",
    "            .to_crs(\"EPSG:4326\")\n",
    "            .geometry[0]\n",
    "            .coords[0]\n",
    "        )\n",
    "        coords.append((lat, lon))\n",
    "    best_coords.append(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ef4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rightmove_client = rightmove.api.Rightmove()\n",
    "all_property_ids = set()\n",
    "for coord, coords in tqdm.tqdm(zip(min_times, best_coords), total=len(best_coords)):\n",
    "    if len(coords) > 1000:\n",
    "        raise ValueError(\"Rightmove accepts polygons with up to 1000 points.\")\n",
    "    search_results, count = await rightmove_client.map_search(\n",
    "        rightmove.api.SearchQuery(\n",
    "            location_identifier=rightmove.api.polyline_identifier(coords),\n",
    "            is_fetching=True,\n",
    "            view_type=\"MAP\",\n",
    "            channel=\"RENT\",\n",
    "            max_price=3000,\n",
    "        )\n",
    "    )\n",
    "    all_property_ids.update(property.id for property in search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_property_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rightmove.models\n",
    "\n",
    "property_results: list[rightmove.models.Property] = []\n",
    "with tqdm.tqdm(total=len(all_property_ids)) as pbar:\n",
    "    for apids in itertools.batched(all_property_ids, 25):\n",
    "        property_results.extend(\n",
    "            await rightmove_client.search_by_ids(apids, channel=\"RENT\")\n",
    "        )\n",
    "        pbar.update(len(apids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b748a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_property_size(property: rightmove.models.Property, min_square_meters: float):\n",
    "    if property.display_size:\n",
    "        if property.display_size.endswith(\" sq. ft.\"):\n",
    "            square_ft = int(\n",
    "                property.display_size.removesuffix(\" sq. ft.\").replace(\",\", \"\")\n",
    "            )\n",
    "            square_meters = int(square_ft * 0.092903)\n",
    "            if square_meters < min_square_meters:\n",
    "                return False\n",
    "        elif property.display_size.endswith(\" sqm\"):\n",
    "            square_meters = int(\n",
    "                property.display_size.removesuffix(\" sqm\").replace(\",\", \"\")\n",
    "            )\n",
    "            if square_meters < min_square_meters:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUDGET = 2250\n",
    "urls = [\n",
    "    rightmove.api.property_url(property.property_url)\n",
    "    for property in property_results\n",
    "    if property.property_url is not None\n",
    "    and check_property_size(property, 60)\n",
    "    and 1900 < rightmove.price.normalize(property.price) <= BUDGET\n",
    "    and (property.number_of_images or 0) > 2\n",
    "    and (property.number_of_floorplans or 0) > 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_browser = False\n",
    "for url in urls:\n",
    "    if open_browser:\n",
    "        webbrowser.open(url)\n",
    "    print(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flathunt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
